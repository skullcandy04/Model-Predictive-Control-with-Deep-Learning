{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3yx4xOgKm_l"
      },
      "source": [
        "# ***Important***\n",
        "\n",
        "**Before starting, make sure to read the [Assignment Instructions](https://courseworks2.columbia.edu/courses/172081/pages/assignment-instructions) page on Courseworks2 to learn the workflow for completing this project.**\n",
        "\n",
        "<ins> Different from project 1 and 2</ins>, apart from the link to your notebook, you are also required to submit 2 additional files namely the collected data `data.pkl` and your chosen model checkpoint `dynamic.pth`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCleXWNaK8ef"
      },
      "source": [
        "## Project Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xm8CaL4aY2am",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1724faa4-a245-4772-c8e6-a0e67009298b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mecs6616_sp23_project4'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 15 (delta 5), reused 6 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (15/15), 8.63 KiB | 1.44 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "# After running this cell, the folder 'mecs6616_sp23_project3' will show up in the file explorer on the left (click on the folder icon if it's not open)\n",
        "# It may take a few seconds to appear\n",
        "!git clone https://github.com/roamlab/mecs6616_sp23_project4.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "# move all needed files into the working directory. This is simply to make accessing files easier\n",
        "!mv /content/mecs6616_sp23_project4/* /content/"
      ],
      "metadata": {
        "id": "KVOBvh6GvAgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8Cj2R4NyzSg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e369cd5-6b2d-4392-e858-78477a2fc20e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ray\n",
            "  Downloading ray-2.3.1-cp39-cp39-manylinux2014_x86_64.whl (58.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.9/dist-packages (from ray) (22.2.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.9/dist-packages (from ray) (4.3.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from ray) (2.27.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from ray) (1.0.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from ray) (3.11.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from ray) (8.1.3)\n",
            "Collecting aiosignal\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: grpcio>=1.32.0 in /usr/local/lib/python3.9/dist-packages (from ray) (1.53.0)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.9/dist-packages (from ray) (1.22.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from ray) (6.0)\n",
            "Collecting virtualenv>=20.0.24\n",
            "  Downloading virtualenv-20.21.0-py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.9/dist-packages (from ray) (3.20.3)\n",
            "Collecting frozenlist\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting distlib<1,>=0.3.6\n",
            "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs<4,>=2.4 in /usr/local/lib/python3.9/dist-packages (from virtualenv>=20.0.24->ray) (3.2.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema->ray) (0.19.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->ray) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->ray) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->ray) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->ray) (2022.12.7)\n",
            "Installing collected packages: distlib, virtualenv, frozenlist, aiosignal, ray\n",
            "Successfully installed aiosignal-1.3.1 distlib-0.3.6 frozenlist-1.3.3 ray-2.3.1 virtualenv-20.21.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ray"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIoNAwGQpHfH"
      },
      "source": [
        "# Starter Code Explanation\n",
        "\n",
        "This project uses the same environment as in project 3 which consists of an n linked robot arm. We provide code for teacher dynamics and the controller. We also provide scripts for collecting data, skeleton classes for implementing student dynamics and finally training and evaluating the model.\n",
        "\n",
        "Similar to last time we define a Robot class inside `robot.py` which provides the interface for controlling the robot arm i.e it provides you with some functions to set/get the state and set the action for the arm and take a step using the `Robot.advance()` method. The state of the arm is a 2n-dimensional vector : n joint positions [rad] + n joint velocities [rad/s] and the action is defined as the n torques (in N-m) applied to n joints respectively.\n",
        "\n",
        "In `arm_dynamic_base.py` we define the base class for forward dynamics for the arm. The ground truth forward dynamics are defined in `arm_dynamics_teacher.py`. The student dynamics which internally uses a neural network model is defined in `arm_dynamics_student.py`\n",
        "\n",
        "You are welcome to look in-depth to understand how the ground truth forward dynamics is computed for an arm, given its number of links, link mass, and viscous friction of the environment - however this is not necessary to successfully complete this assignment.\n",
        "\n",
        "\n",
        "The `models.py` file provides the base class for neural network to learn forward model and also contains skeleton code for you to implement the network architecture for arms with different number of links. We will revisit this again in when providing instructions for training below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfEtTP6Gob6m"
      },
      "source": [
        "## Part 1: Implement Model Predictive Control\n",
        "\n",
        "You will implement this controller by completing the MPC class. Specifically, you will implement the compute_action() method by following the algorithm discussed in the lecture. As with previous projects you are free to implement additional methods as needed or change the initialization if need be. While scoring your controller, you will be creating an instance of the MPC class and passing it to the scoring function so ensure that the arguments to the compute_action method remain the same.\n",
        "\n",
        "Although you do not need to understand how the ArmDynamicsTeacher class works, you could use the compute_fk() method from the class. This will allow you to convert from the state value (represented as array of shape (2*n, 1) where n is num_links) to final end effector position (x, y position of the end effector). Similary we can also compute the velocity of the end effector with the code below:\n",
        "```\n",
        "pos_ee = dynamics.compute_fk(state)\n",
        "vel_ee = dynamics.compute_vel_ee(state)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgR69Xg9RsOT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "class MPC:\n",
        "    def __init__(self):\n",
        "        self.control_horizon = 10\n",
        "        self.N = 40\n",
        "\n",
        "    def compute_action(self, dynamics, state, goal, action):\n",
        "      a = 1.2\n",
        "      b = 0.014\n",
        "      c = 0.11\n",
        "      u = 0.01\n",
        "      actions = np.broadcast_to(action, (dynamics.get_action_dim(), self.N))\n",
        "      orig = state\n",
        "      curr = state\n",
        "      cost = 0\n",
        "      pos_ee = np.linalg.norm(dynamics.compute_fk(state))\n",
        "      for i in range(self.N):\n",
        "        new_state = dynamics.dynamics_step(curr, actions[:, i], dynamics.dt)\n",
        "        #new_state = new_state.detach().cpu().numpy().reshape((len(state),1))\n",
        "        pos_ee = dynamics.compute_fk(new_state)\n",
        "        vel_ee = dynamics.compute_vel_ee(new_state)\n",
        "        cost += a*(np.linalg.norm(goal - pos_ee)) + b*np.sum(actions[:, i]) + c*(np.linalg.norm(vel_ee)/(np.linalg.norm(goal - pos_ee)+0.7))\n",
        "        curr = new_state\n",
        "      cost = np.absolute(cost)\n",
        "      curr = orig\n",
        "\n",
        "      for i in range(20):\n",
        "        best_cost = np.inf\n",
        "        best_actions = actions\n",
        "        for j in range(dynamics.get_num_links()):\n",
        "          curr_up = orig\n",
        "          curr_down = orig\n",
        "          cost_up = 0\n",
        "          cost_down = 0\n",
        "          actions_up = np.copy(actions)\n",
        "          actions_down = np.copy(actions)\n",
        "          link_up = np.copy(actions[j, :]) + u\n",
        "          link_down = np.copy(actions[j, :]) - u\n",
        "          actions_up[j, :] = link_up\n",
        "          actions_down[j, :] = link_down\n",
        "          for k in range(self.N):\n",
        "            new_up = dynamics.dynamics_step(curr_up, actions_up[:, k], dynamics.dt)\n",
        "            #new_state_up = new_state_up.detach().cpu().numpy().reshape((len(original_state),1))\n",
        "            pos_up = dynamics.compute_fk(new_up)\n",
        "            new_down = dynamics.dynamics_step(curr_down, actions_down[:, k], dynamics.dt)\n",
        "            #new_state_down = new_state_down.detach().cpu().numpy().reshape((len(original_state),1))\n",
        "            pos_down = dynamics.compute_fk(new_down)\n",
        "            vel_up = dynamics.compute_vel_ee(new_up)\n",
        "            vel_down = dynamics.compute_vel_ee(new_down)\n",
        "            cost_up += a*(np.linalg.norm(goal - pos_up)) + b*np.sum(actions_up[:, k]) + c*(np.linalg.norm(vel_up)/(np.linalg.norm(goal - pos_up)+0.7))\n",
        "            cost_down += a*(np.linalg.norm(goal - pos_down)) + b*np.sum(actions_down[:, k]) + c*(np.linalg.norm(vel_down)/(np.linalg.norm(goal - pos_down)+0.7))\n",
        "            curr_up = new_up\n",
        "            curr_down = new_down\n",
        "\n",
        "          cost_up = np.absolute(cost_up)\n",
        "          cost_down = np.absolute(cost_down)\n",
        "\n",
        "          if cost_up < cost_down:\n",
        "            if cost_up < best_cost:\n",
        "              best_cost = cost_up\n",
        "              best_actions = actions_up\n",
        "          elif cost_up > cost_down:\n",
        "            if cost_down < best_cost:\n",
        "              best_cost = cost_down\n",
        "              best_actions = actions_down\n",
        "        if best_cost < cost:\n",
        "          cost = best_cost\n",
        "          actions = best_actions\n",
        "\n",
        "      return np.reshape(actions[:, 0], (dynamics.get_num_links(), 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br5wyFnGRxgg"
      },
      "source": [
        "# Manually testing the controller\n",
        "This part is for you to manually check the performance of your controller before you are ready for it be evaluated by our scoring function.\n",
        "To test your implementation run the following code. Feel free to play around with the cell or change the num_links / goal positions . You can define your controller however you would like to and then switch on gui to see how close your end effectors get to the goal position\n",
        "\n",
        "Every time step within the environment is 0.01s, which is defined in the dynamics as `dt`.\n",
        "\n",
        "The MPC class has a `control_horizon` variable which represents the frequency at which `controller.compute_action()` will be called\n",
        "\n",
        "In the scoring function you will be evaluated on the distance of your end effector to the goal position and the velocity of the end effector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ0GFCoLpJ9A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3417b31a-ca13-4d32-f02f-cc689e937145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At timestep 0: Distance to goal: 3.3970575502926055, Velocity of end effector: 5.539171676143414e-18\n",
            "At timestep 10: Distance to goal: 3.3700694398297704, Velocity of end effector: 0.4711885209845729\n",
            "At timestep 20: Distance to goal: 3.2917972065334293, Velocity of end effector: 0.8281741754765136\n",
            "At timestep 30: Distance to goal: 3.1525007932809657, Velocity of end effector: 1.0354721566284708\n",
            "At timestep 40: Distance to goal: 2.947066456960029, Velocity of end effector: 1.11947025322798\n",
            "At timestep 50: Distance to goal: 2.6834354436087184, Velocity of end effector: 1.0485247276079983\n",
            "At timestep 60: Distance to goal: 2.4395169554959923, Velocity of end effector: 0.6550021782857365\n",
            "At timestep 70: Distance to goal: 2.29873906214577, Velocity of end effector: 0.29989022638915597\n",
            "At timestep 80: Distance to goal: 2.267473038773757, Velocity of end effector: 0.13787567268440903\n",
            "At timestep 90: Distance to goal: 2.2917608784042995, Velocity of end effector: 0.2123406883827209\n",
            "At timestep 100: Distance to goal: 2.3099799861573262, Velocity of end effector: 0.32874140597421553\n",
            "At timestep 110: Distance to goal: 2.289162834674475, Velocity of end effector: 0.5378691810712506\n",
            "At timestep 120: Distance to goal: 2.215843973280226, Velocity of end effector: 0.7232244360163649\n",
            "At timestep 130: Distance to goal: 2.113809523934495, Velocity of end effector: 1.0039963961739347\n",
            "At timestep 140: Distance to goal: 2.0245889592955595, Velocity of end effector: 0.9963295962525321\n",
            "At timestep 150: Distance to goal: 1.955068818254725, Velocity of end effector: 0.9244113024479048\n",
            "At timestep 160: Distance to goal: 1.8936026314798702, Velocity of end effector: 0.9056811941986741\n",
            "At timestep 170: Distance to goal: 1.8338805393121305, Velocity of end effector: 0.8706840468597735\n",
            "At timestep 180: Distance to goal: 1.7699161119140427, Velocity of end effector: 0.8112306976674992\n",
            "At timestep 190: Distance to goal: 1.693221588399729, Velocity of end effector: 0.8249652921866921\n",
            "At timestep 200: Distance to goal: 1.600615076449375, Velocity of end effector: 0.913291838831034\n",
            "At timestep 210: Distance to goal: 1.5028673272974324, Velocity of end effector: 0.943441880448701\n",
            "At timestep 220: Distance to goal: 1.4155223707620201, Velocity of end effector: 0.923445489676877\n",
            "At timestep 230: Distance to goal: 1.3457748802893394, Velocity of end effector: 0.8264334340075924\n",
            "At timestep 240: Distance to goal: 1.2918094821455004, Velocity of end effector: 0.7157533280503772\n",
            "At timestep 250: Distance to goal: 1.2456124675916334, Velocity of end effector: 0.5828052377644934\n",
            "At timestep 260: Distance to goal: 1.1981783396211159, Velocity of end effector: 0.5249307258947665\n",
            "At timestep 270: Distance to goal: 1.1424467553193571, Velocity of end effector: 0.6253498073268337\n",
            "At timestep 280: Distance to goal: 1.0784465882743464, Velocity of end effector: 0.7164255603245253\n",
            "At timestep 290: Distance to goal: 1.0117440395864108, Velocity of end effector: 0.7358921088531292\n",
            "At timestep 300: Distance to goal: 0.945829299002954, Velocity of end effector: 0.7269737963852541\n",
            "At timestep 310: Distance to goal: 0.8817029173650273, Velocity of end effector: 0.7018972519526189\n",
            "At timestep 320: Distance to goal: 0.8198176612018702, Velocity of end effector: 0.6608756209350701\n",
            "At timestep 330: Distance to goal: 0.7601149724641857, Velocity of end effector: 0.6183541759258268\n",
            "At timestep 340: Distance to goal: 0.7018081934782244, Velocity of end effector: 0.5896760457455763\n",
            "At timestep 350: Distance to goal: 0.6437362508497987, Velocity of end effector: 0.5811210715284645\n",
            "At timestep 360: Distance to goal: 0.5845866451562521, Velocity of end effector: 0.5934479644827657\n",
            "At timestep 370: Distance to goal: 0.5229681939846618, Velocity of end effector: 0.6222125428799937\n",
            "At timestep 380: Distance to goal: 0.458451126595032, Velocity of end effector: 0.6550765753467717\n",
            "At timestep 390: Distance to goal: 0.39098657122636793, Velocity of end effector: 0.6921877457136257\n",
            "At timestep 400: Distance to goal: 0.32303450692297364, Velocity of end effector: 0.6716903260047978\n",
            "At timestep 410: Distance to goal: 0.2606478222069545, Velocity of end effector: 0.5827273147298614\n",
            "At timestep 420: Distance to goal: 0.2085536070744341, Velocity of end effector: 0.46749216527802406\n",
            "At timestep 430: Distance to goal: 0.16732481215737274, Velocity of end effector: 0.3668297363581632\n",
            "At timestep 440: Distance to goal: 0.13515024048879404, Velocity of end effector: 0.2865183538275208\n",
            "At timestep 450: Distance to goal: 0.10993322578147247, Velocity of end effector: 0.2248046158145757\n",
            "At timestep 460: Distance to goal: 0.09005874985494321, Velocity of end effector: 0.17751020428768396\n",
            "At timestep 470: Distance to goal: 0.07421458400073293, Velocity of end effector: 0.14302463941323473\n",
            "At timestep 480: Distance to goal: 0.06143341379606894, Velocity of end effector: 0.10790346332617008\n",
            "At timestep 490: Distance to goal: 0.05126034503172238, Velocity of end effector: 0.08769652872300893\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "from arm_dynamics_teacher import ArmDynamicsTeacher\n",
        "from robot import Robot\n",
        "from render import Renderer\n",
        "from score import *\n",
        "import torch\n",
        "import time\n",
        "import math\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "# Teacher arm with 3 links\n",
        "dynamics_teacher = ArmDynamicsTeacher(\n",
        "    num_links=3,\n",
        "    link_mass=0.1,\n",
        "    link_length=1,\n",
        "    joint_viscous_friction=0.1,\n",
        "    dt=0.01)\n",
        "\n",
        "arm = Robot(dynamics_teacher)\n",
        "arm.reset()\n",
        "\n",
        "gui = False\n",
        "\n",
        "if gui:\n",
        "  renderer = Renderer()\n",
        "  time.sleep(1)\n",
        "\n",
        "# Controller\n",
        "controller = MPC()\n",
        "\n",
        "# Resetting the arm will set its state so that it is in the vertical position,\n",
        "# and set the action to be zeros\n",
        "arm.reset()\n",
        "\n",
        "# Choose the goal position you would like to see the performance of your controller\n",
        "goal = np.zeros((2, 1))\n",
        "goal[0, 0] = 2.5\n",
        "goal[1, 0] = -0.7\n",
        "arm.goal = goal\n",
        "\n",
        "dt = 0.01\n",
        "time_limit = 5\n",
        "num_steps = round(time_limit/dt)\n",
        "\n",
        "# Control loop\n",
        "for s in range(num_steps):\n",
        "  t = time.time()\n",
        "  arm.advance()\n",
        "\n",
        "  if gui:\n",
        "    renderer.plot([(arm, \"tab:blue\")])\n",
        "  time.sleep(max(0, dt - (time.time() - t)))\n",
        "\n",
        "  if s % controller.control_horizon==0:\n",
        "    state = arm.get_state()\n",
        "\n",
        "    # Measuring distance and velocity of end effector\n",
        "    pos_ee = dynamics_teacher.compute_fk(state)\n",
        "    dist = np.linalg.norm(goal-pos_ee)\n",
        "    vel_ee = np.linalg.norm(arm.dynamics.compute_vel_ee(state))\n",
        "    print(f'At timestep {s}: Distance to goal: {dist}, Velocity of end effector: {vel_ee}')\n",
        "\n",
        "    action = arm.action\n",
        "    action = controller.compute_action(arm.dynamics, state, goal, action)\n",
        "    arm.set_action(action)\n",
        "    #print(f'{s}: state: {state.reshape((2*dynamics_teacher.num_links,1))}, action: {action.reshape((dynamics_teacher.num_links,1))}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVIGZZimCNI4"
      },
      "source": [
        "## Grading and Evaluation for Part 1\n",
        "Your controller will be graded on 6 tests. 2 tests each for the 1-link ,2-link, and 3-link arms. The arm will start off in the initial state with the arms pointing stright down. The testing criteria depend on the distance and the velocity of the end effectors . Each test will run the robot arm for **5.0 seconds**. At the end of the 5 seconds the test will be:\n",
        "\n",
        "A success if your end effectors meet this criteria:\n",
        "`distance_to_goal < 0.1 and vel_ee < 0.5`\n",
        "\n",
        "A partial success if your end effectors meet this criteria:\n",
        "`distance_to_goal < 0.2 and vel_ee < 0.5`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXswattzCLyC"
      },
      "outputs": [],
      "source": [
        "# Scoring using score_mpc\n",
        "controller = MPC()\n",
        "gui = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE\n",
        "score_mpc_true_dynamics(controller, gui)"
      ],
      "metadata": {
        "id": "B5iMweqC1SsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d574a5f-9572-4480-acb6-8307b9267051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Part1: EVALUATING CONTROLLER (with perfect dynamics)\n",
            "-----------------------------------------------------\n",
            "NUM_LINKS: 1\n",
            "Test  1\n",
            "Success! :)\n",
            " Goal: [ 0.38941834 -0.92106099], Final position: [ 0.38776116 -0.92175988], Final velocity: [0.]\n",
            "score: 1.5/1.5\n",
            "Test  2\n",
            "Success! :)\n",
            " Goal: [-0.68163876 -0.73168887], Final position: [-0.68358302 -0.72987276], Final velocity: [0.]\n",
            "score: 1.0/1.0\n",
            "NUM_LINKS: 2\n",
            "Test  1\n",
            "Success! :)\n",
            " Goal: [ 0.6814821  -1.61185674], Final position: [ 0.67891771 -1.61451023], Final velocity: [0.00099123]\n",
            "score: 1.5/1.5\n",
            "Test  2\n",
            "Success! :)\n",
            " Goal: [-1.19286783 -1.28045552], Final position: [-1.19033105 -1.28403032], Final velocity: [0.05379889]\n",
            "score: 1.0/1.0\n",
            "NUM_LINKS: 3\n",
            "Test  1\n",
            "Success! :)\n",
            " Goal: [ 1.29444895 -2.36947292], Final position: [ 1.29428909 -2.36853645], Final velocity: [0.00882273]\n",
            "score: 1.5/1.5\n",
            "Test  2\n",
            "Success! :)\n",
            " Goal: [-2.10367746 -1.35075576], Final position: [-2.05807184 -1.37593074], Final velocity: [0.0266228]\n",
            "score: 1.0/1.0\n",
            "       \n",
            "-------------------------\n",
            "Part 1 SCORE:  7.5/7.5\n",
            "-------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O1cAx4rMce7"
      },
      "source": [
        "### Part 2.1: Model Architecture\n",
        "Beyond this point you will be focusing on the 2 link arm only.\n",
        "\n",
        "We have a base class Model and a subclass for the 2-link arm. The class Model is a base class for our models.  In compute_next_state() method,\n",
        "you have to use the trick to use joint accelerations to compute the next state similar to what you did in Project 3.\n",
        "\n",
        "In the `Model2Link` class you will use a neural network to compute the joint accelerations by implementing `compute_qddot()` method. This will take 6 values (2 joint angles, 2 joint velocities and 2 actions applied to the arm) and output 2 joint acceleration values\n",
        "\n",
        "Do not change the arguments for the `__init__()` method even if you do not use them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPOWJZOyMekB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class Model(nn.Module):\n",
        "\tdef __init__(self, num_links, time_step):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.num_links = num_links\n",
        "\t\tself.time_step = time_step\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tqddot = self.compute_qddot(x)\n",
        "\t\tstate = x[:, :2*self.num_links]\n",
        "\t\tnext_state = self.compute_next_state(state, qddot)\n",
        "\t\treturn next_state\n",
        "\n",
        "\tdef compute_next_state(self, state, qddot):\n",
        "\t\t# Your code goes here\n",
        "\t\tnext_state = np.zeros_like(state)\n",
        "\t\tnext_state1 = torch.from_numpy(next_state)\n",
        "\t\tstate1 = state\n",
        "\t\tqddot1 = qddot\n",
        "\t\tnext_state1[:,self.num_links: 2*self.num_links] = state1[:,self.num_links: 2*self.num_links] + qddot1*self.time_step\n",
        "\t\tnext_state1[:,0:self.num_links] = state1[:,0:self.num_links] + state1[:,self.num_links: 2*self.num_links]*self.time_step + 0.5*qddot1*(self.time_step*self.time_step)\n",
        "\t\t#torch.reshape(next_state1, (4,1))\n",
        "\t\treturn next_state1\n",
        "\n",
        "\tdef compute_qddot(self, x):\n",
        "\t\tpass\n",
        "\n",
        "class Model2Link(Model):\n",
        "\tdef __init__(self, time_step):\n",
        "\t\tsuper().__init__(2, time_step)\n",
        "\t\tself.fc1 = nn.Linear(6, 64)\n",
        "\t\tself.fc2 = nn.Linear(64, 64)\n",
        "\t\tself.fc3 = nn.Linear(64, 2)\n",
        "\t\tself.relu = nn.ReLU()\n",
        "    # Your code goes here\n",
        "\n",
        "\tdef compute_qddot(self, x):\n",
        "    # Your code goes here\n",
        "\t\t\tx = self.fc1(x)\n",
        "\t\t\tx = self.relu(x)\n",
        "\t\t\tx = self.fc2(x)\n",
        "\t\t\tx = self.relu(x)\n",
        "\t\t\tx = self.fc3(x)\n",
        "\t\t\treturn x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYr1nkuIUQpj"
      },
      "source": [
        "### Part 2.2: Collect Data\n",
        "Similar to project 3, we will collect data which will be used to learn a forward model for our 2 link robot arm. Once we have learnt a forward model you will be evaluated on your MPC Controller that uses the learnt dynamics model instead of the true dynamics.\n",
        "\n",
        "You can modify the collect_data function or write any of your own functions however you choose to. You will be evaluated on the **2 Link Robot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfli4gFgYA-_"
      },
      "outputs": [],
      "source": [
        "from IPython.core.display import json\n",
        "# Teacher arm with 3 links\n",
        "dynamics_teacher = ArmDynamicsTeacher(\n",
        "    num_links=2,\n",
        "    link_mass=0.1,\n",
        "    link_length=1,\n",
        "    joint_viscous_friction=0.1,\n",
        "    dt=0.01)\n",
        "\n",
        "arm = Robot(dynamics_teacher)\n",
        "arm.reset()\n",
        "\n",
        "def collect_data(arm):\n",
        "\n",
        " # ---\n",
        "  # You code goes here. Replace the X, and Y by your collected data\n",
        "  # Control the arm to collect a dataset for training the forward dynamics.\n",
        "  '''\n",
        "  gui = False\n",
        "\n",
        "  if gui:\n",
        "    renderer = Renderer()\n",
        "  time.sleep(1)\n",
        "\n",
        "  # Controller\n",
        "  controller = MPC()\n",
        "\n",
        "  # Resetting the arm will set its state so that it is in the vertical position,\n",
        "  # and set the action to be zeros\n",
        "  arm.reset()\n",
        "\n",
        "  # Choose the goal position you would like to see the performance of your controller\n",
        "\n",
        "  dt = 0.01\n",
        "  time_limit = 2.5\n",
        "  num_steps = round(time_limit/dt)\n",
        "\n",
        "  num_trials = 10\n",
        "  points = np.zeros((2,num_trials))\n",
        "  xval = np.linspace(0.05,1.95,num = num_trials)\n",
        "  yval =\n",
        "  points[0,:] = xval\n",
        "  points[1,:] = yval\n",
        "\n",
        "  X = np.zeros((num_steps*num_trials, arm.dynamics.get_state_dim() + arm.dynamics.get_action_dim()))\n",
        "  Y = np.zeros((num_steps*num_trials, arm.dynamics.get_state_dim()))\n",
        "\n",
        "  for j in range(num_trials):\n",
        "\n",
        "    goal = np.zeros((2, 1))\n",
        "    goal[0, 0] = points[0,j]\n",
        "    goal[1, 0] = points[1,j]\n",
        "    arm.goal = goal\n",
        "    print(goal)\n",
        "\n",
        "    for s in range(num_steps):\n",
        "      t = time.time()\n",
        "      arm.advance()\n",
        "\n",
        "      if gui:\n",
        "        renderer.plot([(arm, \"tab:blue\")])\n",
        "      time.sleep(max(0, dt - (time.time() - t)))\n",
        "\n",
        "      if s % controller.control_horizon==0:\n",
        "        state = arm.get_state()\n",
        "\n",
        "        # Measuring distance and velocity of end effector\n",
        "        pos_ee = dynamics_teacher.compute_fk(state)\n",
        "        dist = np.linalg.norm(goal-pos_ee)\n",
        "        vel_ee = np.linalg.norm(arm.dynamics.compute_vel_ee(state))\n",
        "        print(f'At timestep {s}: Distance to goal: {dist}, Velocity of end effector: {vel_ee}')\n",
        "\n",
        "        action = arm.action\n",
        "        action = controller.compute_action(arm.dynamics, state, goal, action)\n",
        "        arm.set_action(action)\n",
        "\n",
        "        next_state = arm.dynamics.dynamics_step(state,action,0.01)\n",
        "        k = j*num_steps + s\n",
        "        X[k, :] = np.concatenate((state.flatten(), action.flatten()))\n",
        "        Y[k, :] = next_state.flatten()\n",
        "      #print(f'At timestep {i}: Distance to goal: {dist}, Velocity of end effector: {vel_ee}')\n",
        "      #print(f\"step: {i}, X: {X[i,:]}, Y: {Y[i,:]}\")\n",
        "      #print(f'step {i}')\n",
        "  '''\n",
        "\n",
        "  gui = False\n",
        "\n",
        "  if gui:\n",
        "    renderer = Renderer()\n",
        "  time.sleep(1)\n",
        "\n",
        "  # Controller\n",
        "  controller = MPC()\n",
        "\n",
        "  # Resetting the arm will set its state so that it is in the vertical position,\n",
        "  # and set the action to be zeros\n",
        "  arm.reset()\n",
        "\n",
        "  # Choose the goal position you would like to see the performance of your controller\n",
        "  goal = np.zeros((2, 1))\n",
        "  goal[0, 0] = 1.3\n",
        "  goal[1, 0] = -0.7\n",
        "  arm.goal = goal\n",
        "\n",
        "  dt = 0.01\n",
        "  time_limit = 10\n",
        "  num_steps = round(time_limit/dt)\n",
        "\n",
        "  X = np.zeros((num_steps, arm.dynamics.get_state_dim() + arm.dynamics.get_action_dim()))\n",
        "  Y = np.zeros((num_steps, arm.dynamics.get_state_dim()))\n",
        "  state = np.zeros((4,1))\n",
        "\n",
        "  # Control loop\n",
        "  for i in range(num_steps):\n",
        "    t = time.time()\n",
        "    arm.advance()\n",
        "\n",
        "    if gui:\n",
        "      renderer.plot([(arm, \"tab:blue\")])\n",
        "    time.sleep(max(0, dt - (time.time() - t)))\n",
        "\n",
        "    if i % controller.control_horizon==0:\n",
        "      state = arm.get_state()\n",
        "\n",
        "      # Measuring distance and velocity of end effector\n",
        "      pos_ee = dynamics_teacher.compute_fk(state)\n",
        "      dist = np.linalg.norm(goal-pos_ee)\n",
        "      vel_ee = np.linalg.norm(arm.dynamics.compute_vel_ee(state))\n",
        "      print(f'At timestep {i}: Distance to goal: {dist}, Velocity of end effector: {vel_ee}')\n",
        "\n",
        "      action = arm.action\n",
        "      action = controller.compute_action(arm.dynamics, state, goal, action)\n",
        "      arm.set_action(action)\n",
        "      next_state = arm.dynamics.advance(state,action)\n",
        "      X[i, :] = np.concatenate((state.flatten(), action.flatten()))\n",
        "      Y[i, :] = next_state.flatten()\n",
        "      #print(f\"step: {i}, X: {X[i,:]}, Y: {Y[i,:]}\")\n",
        "\n",
        "\n",
        "  # ---\n",
        "\n",
        "  return X, Y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53Qtg48i9_LZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "028d4799-82f1-4574-a357-b559297e69f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At timestep 0: Distance to goal: 1.8384776310850235, Velocity of end effector: 7.715274834628325e-18\n",
            "At timestep 10: Distance to goal: 1.811591747078342, Velocity of end effector: 0.5176842655249215\n",
            "At timestep 20: Distance to goal: 1.7278930644944233, Velocity of end effector: 1.007222878116306\n",
            "At timestep 30: Distance to goal: 1.578859456883025, Velocity of end effector: 1.2321523186649694\n",
            "At timestep 40: Distance to goal: 1.3757853181078932, Velocity of end effector: 1.2785670905159776\n",
            "At timestep 50: Distance to goal: 1.1633500716453227, Velocity of end effector: 0.9305124928093426\n",
            "At timestep 60: Distance to goal: 0.9960983287667095, Velocity of end effector: 0.44022879617422933\n",
            "At timestep 70: Distance to goal: 0.8930275306380792, Velocity of end effector: 0.0745485646324283\n",
            "At timestep 80: Distance to goal: 0.8203843912969085, Velocity of end effector: 0.630135848327024\n",
            "At timestep 90: Distance to goal: 0.7319246624281275, Velocity of end effector: 1.2201930135838297\n",
            "At timestep 100: Distance to goal: 0.599503108956839, Velocity of end effector: 1.772476979290516\n",
            "At timestep 110: Distance to goal: 0.4273076997976875, Velocity of end effector: 1.9316285779179436\n",
            "At timestep 120: Distance to goal: 0.2464717623559384, Velocity of end effector: 1.744912686883632\n",
            "At timestep 130: Distance to goal: 0.09405144356738374, Velocity of end effector: 1.2701996762953143\n",
            "At timestep 140: Distance to goal: 0.008081861501239573, Velocity of end effector: 0.6953494062513261\n",
            "At timestep 150: Distance to goal: 0.05241300752323096, Velocity of end effector: 0.23084597285926312\n",
            "At timestep 160: Distance to goal: 0.059776517974888047, Velocity of end effector: 0.07737350169742561\n",
            "At timestep 170: Distance to goal: 0.04302217922865966, Velocity of end effector: 0.24110457107355387\n",
            "At timestep 180: Distance to goal: 0.02291087922175727, Velocity of end effector: 0.2716576294046082\n",
            "At timestep 190: Distance to goal: 0.028857453664587293, Velocity of end effector: 0.19188505602538683\n",
            "At timestep 200: Distance to goal: 0.037641205144937, Velocity of end effector: 0.08539518315299917\n",
            "At timestep 210: Distance to goal: 0.037955266966999024, Velocity of end effector: 0.017524969180945153\n",
            "At timestep 220: Distance to goal: 0.03388945870673547, Velocity of end effector: 0.012369306119965597\n",
            "At timestep 230: Distance to goal: 0.028074426050045787, Velocity of end effector: 0.03249300368275039\n",
            "At timestep 240: Distance to goal: 0.022262397442888958, Velocity of end effector: 0.026299339677436313\n",
            "At timestep 250: Distance to goal: 0.017617558174829955, Velocity of end effector: 0.032846916213940106\n",
            "At timestep 260: Distance to goal: 0.013608652509659173, Velocity of end effector: 0.03533106763520265\n",
            "At timestep 270: Distance to goal: 0.010017228512183706, Velocity of end effector: 0.033935944464140236\n",
            "At timestep 280: Distance to goal: 0.006950549928420383, Velocity of end effector: 0.02915537816928532\n",
            "At timestep 290: Distance to goal: 0.004540935070607488, Velocity of end effector: 0.02175550910169388\n",
            "At timestep 300: Distance to goal: 0.0024407072230202328, Velocity of end effector: 0.019832813053557396\n",
            "At timestep 310: Distance to goal: 0.00040395437311662146, Velocity of end effector: 0.01603560685655786\n",
            "At timestep 320: Distance to goal: 0.0009565556516354314, Velocity of end effector: 0.010041372076981957\n",
            "At timestep 330: Distance to goal: 0.0016680886221719197, Velocity of end effector: 0.004525264276794996\n",
            "At timestep 340: Distance to goal: 0.0017663104658858532, Velocity of end effector: 0.001988231362521104\n",
            "At timestep 350: Distance to goal: 0.0016932187765836962, Velocity of end effector: 0.0003285118175252231\n",
            "At timestep 360: Distance to goal: 0.0014332008143391286, Velocity of end effector: 0.00570643221650071\n",
            "At timestep 370: Distance to goal: 0.0011588469093487342, Velocity of end effector: 0.0025845291494515773\n",
            "At timestep 380: Distance to goal: 0.0012445157343152966, Velocity of end effector: 0.00044132774783448746\n",
            "At timestep 390: Distance to goal: 0.0008966016257184657, Velocity of end effector: 0.001707106991325565\n",
            "At timestep 400: Distance to goal: 0.001081163383810716, Velocity of end effector: 0.0026854787625800593\n",
            "At timestep 410: Distance to goal: 0.000781806269602142, Velocity of end effector: 0.0005222191556290278\n",
            "At timestep 420: Distance to goal: 0.0010903720660361794, Velocity of end effector: 0.0046662099878532265\n",
            "At timestep 430: Distance to goal: 0.00039087028978454307, Velocity of end effector: 0.0017080118990230211\n",
            "At timestep 440: Distance to goal: 0.0002407903612123713, Velocity of end effector: 0.0021277250301733304\n",
            "At timestep 450: Distance to goal: 0.00026124404184181863, Velocity of end effector: 0.0026737340617162568\n",
            "At timestep 460: Distance to goal: 0.00023146079233259166, Velocity of end effector: 0.001260296339798159\n",
            "At timestep 470: Distance to goal: 0.00022333409064531496, Velocity of end effector: 0.003351870935634106\n",
            "At timestep 480: Distance to goal: 0.00012650856921320066, Velocity of end effector: 0.0008269701048818354\n",
            "At timestep 490: Distance to goal: 0.00023898486958558776, Velocity of end effector: 0.003509977697435576\n",
            "At timestep 500: Distance to goal: 0.00045920061420779497, Velocity of end effector: 0.0009415719332008051\n",
            "At timestep 510: Distance to goal: 0.0006763403075649577, Velocity of end effector: 0.003152837608313793\n",
            "At timestep 520: Distance to goal: 0.000981494794373957, Velocity of end effector: 0.0014916085370776074\n",
            "At timestep 530: Distance to goal: 0.0012230235406297367, Velocity of end effector: 0.002480920029355414\n",
            "At timestep 540: Distance to goal: 0.0006660191337326342, Velocity of end effector: 0.0013707381657534683\n",
            "At timestep 550: Distance to goal: 0.000886677955919821, Velocity of end effector: 0.0014370830638077345\n",
            "At timestep 560: Distance to goal: 0.000657993327468459, Velocity of end effector: 0.0031109256051285034\n",
            "At timestep 570: Distance to goal: 0.001034846486775095, Velocity of end effector: 0.001080373660406975\n",
            "At timestep 580: Distance to goal: 0.00042901405517136565, Velocity of end effector: 0.0024417639350474835\n",
            "At timestep 590: Distance to goal: 0.000505399948223101, Velocity of end effector: 0.0007620289405027681\n",
            "At timestep 600: Distance to goal: 0.0008246213558520531, Velocity of end effector: 0.0025185775635240128\n",
            "At timestep 610: Distance to goal: 0.0010472748458425281, Velocity of end effector: 0.0009050046705063871\n",
            "At timestep 620: Distance to goal: 0.0001218313782725435, Velocity of end effector: 0.003044186543764178\n",
            "At timestep 630: Distance to goal: 0.0008968408471608853, Velocity of end effector: 0.0016877184994548936\n",
            "At timestep 640: Distance to goal: 0.00024840218647174164, Velocity of end effector: 0.0014008850497783884\n",
            "At timestep 650: Distance to goal: 0.0001170661193647772, Velocity of end effector: 0.0020903723171480522\n",
            "At timestep 660: Distance to goal: 0.00037595852932654616, Velocity of end effector: 0.0010528899695070306\n",
            "At timestep 670: Distance to goal: 0.00047076358761770023, Velocity of end effector: 0.0023446946207301956\n",
            "At timestep 680: Distance to goal: 0.000543413615117685, Velocity of end effector: 0.0009263822428714327\n",
            "At timestep 690: Distance to goal: 0.0005173028688688186, Velocity of end effector: 0.0023362745322052032\n",
            "At timestep 700: Distance to goal: 0.0005103553507885197, Velocity of end effector: 0.0010653756355445483\n",
            "At timestep 710: Distance to goal: 0.00046049371277417266, Velocity of end effector: 0.002087157337281859\n",
            "At timestep 720: Distance to goal: 0.0004510637897312629, Velocity of end effector: 0.001393382436450638\n",
            "At timestep 730: Distance to goal: 0.00046890314095881467, Velocity of end effector: 0.0017152937546148965\n",
            "At timestep 740: Distance to goal: 0.0005194392693046992, Velocity of end effector: 0.0017676402727837806\n",
            "At timestep 750: Distance to goal: 0.0006406747565882888, Velocity of end effector: 0.0013718366104443693\n",
            "At timestep 760: Distance to goal: 0.0007551952963574686, Velocity of end effector: 0.002045187878088381\n",
            "At timestep 770: Distance to goal: 0.0009465907899737549, Velocity of end effector: 0.0011780034355931008\n",
            "At timestep 780: Distance to goal: 0.0011104801856201554, Velocity of end effector: 0.002137624350380413\n",
            "At timestep 790: Distance to goal: 0.00031138855910710665, Velocity of end effector: 0.0020305282549120773\n",
            "At timestep 800: Distance to goal: 0.0007926345926102547, Velocity of end effector: 0.0023854756172403753\n",
            "At timestep 810: Distance to goal: 0.00023002259523588657, Velocity of end effector: 0.0010674623112986359\n",
            "At timestep 820: Distance to goal: 0.0006720761635322496, Velocity of end effector: 0.002055076833733521\n",
            "At timestep 830: Distance to goal: 0.00026538536441893034, Velocity of end effector: 0.0022724958889788647\n",
            "At timestep 840: Distance to goal: 0.0006994911501331955, Velocity of end effector: 0.0020314366085652717\n",
            "At timestep 850: Distance to goal: 0.00034189687661024355, Velocity of end effector: 0.0014807130485536538\n",
            "At timestep 860: Distance to goal: 0.0010329081180633054, Velocity of end effector: 0.0016295674812355078\n",
            "At timestep 870: Distance to goal: 0.0003729960364053665, Velocity of end effector: 0.0026654396913721613\n",
            "At timestep 880: Distance to goal: 0.00026254315416011074, Velocity of end effector: 0.0017152616312188108\n",
            "At timestep 890: Distance to goal: 0.000459891063145912, Velocity of end effector: 0.0025400393048081032\n",
            "At timestep 900: Distance to goal: 0.0005351756547652082, Velocity of end effector: 0.0018743961285869707\n",
            "At timestep 910: Distance to goal: 0.0006284136517249239, Velocity of end effector: 0.0023475011748724066\n",
            "At timestep 920: Distance to goal: 0.0005860578860226702, Velocity of end effector: 0.0020837794515618326\n",
            "At timestep 930: Distance to goal: 0.0005937637651562865, Velocity of end effector: 0.002136807177349535\n",
            "At timestep 940: Distance to goal: 0.0004964879885693103, Velocity of end effector: 0.002278597501046434\n",
            "At timestep 950: Distance to goal: 0.00046180410448747354, Velocity of end effector: 0.0019749213767241277\n",
            "At timestep 960: Distance to goal: 0.0003568871518319518, Velocity of end effector: 0.002397568048271195\n",
            "At timestep 970: Distance to goal: 0.0003159221715248912, Velocity of end effector: 0.001909081058588549\n",
            "At timestep 980: Distance to goal: 0.0002952502167924321, Velocity of end effector: 0.002411451948328548\n",
            "At timestep 990: Distance to goal: 0.00032292481876735783, Velocity of end effector: 0.0019476735394187968\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# Call the function you have defined above to collect data\n",
        "X, Y = collect_data(arm)\n",
        "save_dir = 'dataset'\n",
        "if not os.path.exists(save_dir):\n",
        "  os.makedirs(save_dir)\n",
        "\n",
        "# Save the collected data in the data.pkl file\n",
        "data = {'X': X, 'Y': Y}\n",
        "pickle.dump(data, open(os.path.join(save_dir, 'data.pkl'), \"wb\" ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drBZgkLxf-O-"
      },
      "source": [
        "### Part 2.3: Training the forward model\n",
        "By now you would be familiar with the basic skeleton of training a forward model.\n",
        "\n",
        "The starter code already creates the dataset class and loads the dataset with a random 0.8/0.2 train/test split for you. This script should save the model that it trains. You should use a specific procedure for saving, outlined below.\n",
        "\n",
        "In machine learning, it is a very good practice to save not only the final model but also the checkpoints. Our starter code already configures save_dir for you and for each epoch of your training, you should use the following code to save the model at that epoch.\n",
        "\n",
        "```\n",
        "model_folder_name = f'epoch_{epoch:04d}_loss_{test_loss:.8f}'\n",
        "if not os.path.exists(os.path.join(args.save_dir, model_folder_name)):\n",
        "    os.makedirs(os.path.join(args.save_dir, model_folder_name))\n",
        "torch.save(model.state_dict(), os.path.join(args.save_dir, model_folder_name, 'dynamics.pth'))\n",
        "print(f'model saved to {os.path.join(args.save_dir, model_folder_name, \"dynamics.pth\")}\\n')\n",
        "```\n",
        "The output from running this code should be a folder as below:\n",
        "\n",
        "```\n",
        "models/\n",
        "    2021-03-24_23-57-50/\n",
        "        epoch_0001_loss_0.00032930/\n",
        "            dynamics.pth\n",
        "        epoch_0002_loss_0.00009413/\n",
        "            dynamics.pth   \n",
        "        ...  \n",
        "```\n",
        "You can implement the functions below as you please to collect data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c1A-JN4gCtN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tqdm\n",
        "import pickle\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "import time\n",
        "\n",
        "class DynamicDataset(Dataset):\n",
        "  def __init__(self, datafile):\n",
        "    data = pickle.load(open(datafile, 'rb'))\n",
        "    # X: (N, 6), Y: (N, 4)\n",
        "    self.X = data['X'].astype(np.float32)\n",
        "    self.Y = data['Y'].astype(np.float32)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.X.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx], self.Y[idx]\n",
        "\n",
        "\n",
        "def train_one_epoch(model, train_loader, optimizer, criterion):\n",
        "  model.train()\n",
        "\t# ---\n",
        "\t# Your code goes here\n",
        "  train_loss = 0.0\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "      optimizer.zero_grad()\n",
        "      output = model(data)\n",
        "      loss = criterion(output, target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      train_loss += loss.item()\n",
        "  return train_loss / len(train_loader)\n",
        "\n",
        "\t# ---\n",
        "\n",
        "\n",
        "def test(model, test_loader, criterion):\n",
        "  model.eval()\n",
        "\t# --\n",
        "\t# Your code goes here\n",
        "  test_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (data, target) in enumerate(test_loader):\n",
        "      #data, target = data.to(device), target.to(device)\n",
        "      output = model(data)\n",
        "      loss = criterion(output, target)\n",
        "      test_loss += loss.item()\n",
        "  return test_loss / len(test_loader)\n",
        "  # --\n",
        "\n",
        "def train_forward_model():\n",
        "\n",
        "\t# --\n",
        "\t# Implement this function\n",
        "  # --\n",
        "\n",
        "  # Keep track of the checkpoint with the smallest test loss and save in model_path\n",
        "  model_path = None\n",
        "  max_test_loss = 1e4\n",
        "  model = Model2Link(0.01)\n",
        "\n",
        "  datafile = 'dataset/data.pkl'\n",
        "  split = 0.2\n",
        "  dataset = DynamicDataset(datafile)\n",
        "  dataset_size = len(dataset)\n",
        "  test_size = int(np.floor(split * dataset_size))\n",
        "  train_size = dataset_size - test_size\n",
        "  train_set, test_set = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_set, batch_size=100, shuffle=True)\n",
        "  test_loader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=True)\n",
        "\n",
        "  # The name of the directory to save all the checkpoints\n",
        "  timestr = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "  model_dir = os.path.join('models', timestr)\n",
        "\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "  criterion = nn.MSELoss()\n",
        "\n",
        "  epochs=100\n",
        "  for epoch in range(1, 1 + epochs):\n",
        "    # --\n",
        "    # Your code goes here\n",
        "    # Train one epoch\n",
        "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
        "\n",
        "        # Test the model\n",
        "        test_loss = test(model, test_loader, criterion)\n",
        "\n",
        "        folder = f'epoch_{epoch:04d}_loss_{test_loss:.8f}'\n",
        "        if not os.path.exists(os.path.join(model_dir, folder)):\n",
        "          os.makedirs(os.path.join(model_dir, folder))\n",
        "        torch.save(model.state_dict(), os.path.join(model_dir, folder, 'dynamics.pth'))\n",
        "\n",
        "        if model_path is None or test_loss < max_test_loss:\n",
        "          max_test_loss = test_loss\n",
        "          model_path = os.path.join(model_dir, folder, 'dynamics.pth')\n",
        "          print(model_path)\n",
        "\n",
        "        print(f'Epoch [{epoch}/{epochs}] Train Loss: {train_loss:.11f} Test Loss: {test_loss:.11f}')\n",
        "    # --\n",
        "\n",
        "\n",
        "  return model_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4g5oanOA2lJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f3e2ec8-77f8-465e-a1a9-7d1601a6dcf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/2023-04-16_03-52-07/epoch_0001_loss_0.00001968/dynamics.pth\n",
            "Epoch [1/100] Train Loss: 0.00010953208 Test Loss: 0.00001968396\n",
            "models/2023-04-16_03-52-07/epoch_0002_loss_0.00001948/dynamics.pth\n",
            "Epoch [2/100] Train Loss: 0.00010843930 Test Loss: 0.00001947767\n",
            "models/2023-04-16_03-52-07/epoch_0003_loss_0.00001941/dynamics.pth\n",
            "Epoch [3/100] Train Loss: 0.00010795691 Test Loss: 0.00001941152\n",
            "models/2023-04-16_03-52-07/epoch_0004_loss_0.00001938/dynamics.pth\n",
            "Epoch [4/100] Train Loss: 0.00010769417 Test Loss: 0.00001938397\n",
            "Epoch [5/100] Train Loss: 0.00010740867 Test Loss: 0.00001938963\n",
            "models/2023-04-16_03-52-07/epoch_0006_loss_0.00001919/dynamics.pth\n",
            "Epoch [6/100] Train Loss: 0.00010683773 Test Loss: 0.00001919491\n",
            "models/2023-04-16_03-52-07/epoch_0007_loss_0.00001914/dynamics.pth\n",
            "Epoch [7/100] Train Loss: 0.00010642738 Test Loss: 0.00001914479\n",
            "models/2023-04-16_03-52-07/epoch_0008_loss_0.00001910/dynamics.pth\n",
            "Epoch [8/100] Train Loss: 0.00010594879 Test Loss: 0.00001910398\n",
            "models/2023-04-16_03-52-07/epoch_0009_loss_0.00001898/dynamics.pth\n",
            "Epoch [9/100] Train Loss: 0.00010532680 Test Loss: 0.00001897845\n",
            "models/2023-04-16_03-52-07/epoch_0010_loss_0.00001887/dynamics.pth\n",
            "Epoch [10/100] Train Loss: 0.00010477289 Test Loss: 0.00001886717\n",
            "models/2023-04-16_03-52-07/epoch_0011_loss_0.00001871/dynamics.pth\n",
            "Epoch [11/100] Train Loss: 0.00010395448 Test Loss: 0.00001871003\n",
            "models/2023-04-16_03-52-07/epoch_0012_loss_0.00001859/dynamics.pth\n",
            "Epoch [12/100] Train Loss: 0.00010300482 Test Loss: 0.00001858904\n",
            "models/2023-04-16_03-52-07/epoch_0013_loss_0.00001822/dynamics.pth\n",
            "Epoch [13/100] Train Loss: 0.00010230561 Test Loss: 0.00001822479\n",
            "models/2023-04-16_03-52-07/epoch_0014_loss_0.00001803/dynamics.pth\n",
            "Epoch [14/100] Train Loss: 0.00010102982 Test Loss: 0.00001803291\n",
            "models/2023-04-16_03-52-07/epoch_0015_loss_0.00001787/dynamics.pth\n",
            "Epoch [15/100] Train Loss: 0.00010032169 Test Loss: 0.00001786517\n",
            "models/2023-04-16_03-52-07/epoch_0016_loss_0.00001740/dynamics.pth\n",
            "Epoch [16/100] Train Loss: 0.00009874274 Test Loss: 0.00001739828\n",
            "models/2023-04-16_03-52-07/epoch_0017_loss_0.00001682/dynamics.pth\n",
            "Epoch [17/100] Train Loss: 0.00009709451 Test Loss: 0.00001682407\n",
            "models/2023-04-16_03-52-07/epoch_0018_loss_0.00001661/dynamics.pth\n",
            "Epoch [18/100] Train Loss: 0.00009581925 Test Loss: 0.00001661103\n",
            "models/2023-04-16_03-52-07/epoch_0019_loss_0.00001620/dynamics.pth\n",
            "Epoch [19/100] Train Loss: 0.00009402523 Test Loss: 0.00001620064\n",
            "models/2023-04-16_03-52-07/epoch_0020_loss_0.00001519/dynamics.pth\n",
            "Epoch [20/100] Train Loss: 0.00009208506 Test Loss: 0.00001518757\n",
            "models/2023-04-16_03-52-07/epoch_0021_loss_0.00001467/dynamics.pth\n",
            "Epoch [21/100] Train Loss: 0.00008985067 Test Loss: 0.00001466579\n",
            "models/2023-04-16_03-52-07/epoch_0022_loss_0.00001391/dynamics.pth\n",
            "Epoch [22/100] Train Loss: 0.00008766838 Test Loss: 0.00001391167\n",
            "models/2023-04-16_03-52-07/epoch_0023_loss_0.00001273/dynamics.pth\n",
            "Epoch [23/100] Train Loss: 0.00008538495 Test Loss: 0.00001272532\n",
            "models/2023-04-16_03-52-07/epoch_0024_loss_0.00001240/dynamics.pth\n",
            "Epoch [24/100] Train Loss: 0.00008250067 Test Loss: 0.00001239707\n",
            "models/2023-04-16_03-52-07/epoch_0025_loss_0.00001120/dynamics.pth\n",
            "Epoch [25/100] Train Loss: 0.00007965481 Test Loss: 0.00001120343\n",
            "models/2023-04-16_03-52-07/epoch_0026_loss_0.00001071/dynamics.pth\n",
            "Epoch [26/100] Train Loss: 0.00007733882 Test Loss: 0.00001070741\n",
            "models/2023-04-16_03-52-07/epoch_0027_loss_0.00000885/dynamics.pth\n",
            "Epoch [27/100] Train Loss: 0.00007382971 Test Loss: 0.00000884796\n",
            "models/2023-04-16_03-52-07/epoch_0028_loss_0.00000813/dynamics.pth\n",
            "Epoch [28/100] Train Loss: 0.00006970280 Test Loss: 0.00000813344\n",
            "models/2023-04-16_03-52-07/epoch_0029_loss_0.00000713/dynamics.pth\n",
            "Epoch [29/100] Train Loss: 0.00006614232 Test Loss: 0.00000713360\n",
            "models/2023-04-16_03-52-07/epoch_0030_loss_0.00000580/dynamics.pth\n",
            "Epoch [30/100] Train Loss: 0.00006404933 Test Loss: 0.00000579901\n",
            "models/2023-04-16_03-52-07/epoch_0031_loss_0.00000561/dynamics.pth\n",
            "Epoch [31/100] Train Loss: 0.00006037672 Test Loss: 0.00000560738\n",
            "models/2023-04-16_03-52-07/epoch_0032_loss_0.00000417/dynamics.pth\n",
            "Epoch [32/100] Train Loss: 0.00005581166 Test Loss: 0.00000417247\n",
            "models/2023-04-16_03-52-07/epoch_0033_loss_0.00000345/dynamics.pth\n",
            "Epoch [33/100] Train Loss: 0.00005320617 Test Loss: 0.00000345341\n",
            "models/2023-04-16_03-52-07/epoch_0034_loss_0.00000260/dynamics.pth\n",
            "Epoch [34/100] Train Loss: 0.00004843222 Test Loss: 0.00000260262\n",
            "models/2023-04-16_03-52-07/epoch_0035_loss_0.00000181/dynamics.pth\n",
            "Epoch [35/100] Train Loss: 0.00004556702 Test Loss: 0.00000181477\n",
            "models/2023-04-16_03-52-07/epoch_0036_loss_0.00000145/dynamics.pth\n",
            "Epoch [36/100] Train Loss: 0.00004205806 Test Loss: 0.00000144864\n",
            "models/2023-04-16_03-52-07/epoch_0037_loss_0.00000111/dynamics.pth\n",
            "Epoch [37/100] Train Loss: 0.00003840403 Test Loss: 0.00000110790\n",
            "models/2023-04-16_03-52-07/epoch_0038_loss_0.00000073/dynamics.pth\n",
            "Epoch [38/100] Train Loss: 0.00003575304 Test Loss: 0.00000073166\n",
            "models/2023-04-16_03-52-07/epoch_0039_loss_0.00000050/dynamics.pth\n",
            "Epoch [39/100] Train Loss: 0.00003372500 Test Loss: 0.00000049943\n",
            "models/2023-04-16_03-52-07/epoch_0040_loss_0.00000040/dynamics.pth\n",
            "Epoch [40/100] Train Loss: 0.00003006202 Test Loss: 0.00000040033\n",
            "Epoch [41/100] Train Loss: 0.00002770261 Test Loss: 0.00000044515\n",
            "models/2023-04-16_03-52-07/epoch_0042_loss_0.00000035/dynamics.pth\n",
            "Epoch [42/100] Train Loss: 0.00002610274 Test Loss: 0.00000035268\n",
            "Epoch [43/100] Train Loss: 0.00002298654 Test Loss: 0.00000050610\n",
            "Epoch [44/100] Train Loss: 0.00002201240 Test Loss: 0.00000074405\n",
            "Epoch [45/100] Train Loss: 0.00001934707 Test Loss: 0.00000077922\n",
            "Epoch [46/100] Train Loss: 0.00001827812 Test Loss: 0.00000116951\n",
            "Epoch [47/100] Train Loss: 0.00001660586 Test Loss: 0.00000105722\n",
            "Epoch [48/100] Train Loss: 0.00001465003 Test Loss: 0.00000121720\n",
            "Epoch [49/100] Train Loss: 0.00001377158 Test Loss: 0.00000144590\n",
            "Epoch [50/100] Train Loss: 0.00001252034 Test Loss: 0.00000158037\n",
            "Epoch [51/100] Train Loss: 0.00001172639 Test Loss: 0.00000159387\n",
            "Epoch [52/100] Train Loss: 0.00001027278 Test Loss: 0.00000176762\n",
            "Epoch [53/100] Train Loss: 0.00000947181 Test Loss: 0.00000206795\n",
            "Epoch [54/100] Train Loss: 0.00000869064 Test Loss: 0.00000193813\n",
            "Epoch [55/100] Train Loss: 0.00000795103 Test Loss: 0.00000179707\n",
            "Epoch [56/100] Train Loss: 0.00000760184 Test Loss: 0.00000211430\n",
            "Epoch [57/100] Train Loss: 0.00000688726 Test Loss: 0.00000204843\n",
            "Epoch [58/100] Train Loss: 0.00000657632 Test Loss: 0.00000241818\n",
            "Epoch [59/100] Train Loss: 0.00000606080 Test Loss: 0.00000172826\n",
            "Epoch [60/100] Train Loss: 0.00000527078 Test Loss: 0.00000199473\n",
            "Epoch [61/100] Train Loss: 0.00000506251 Test Loss: 0.00000176890\n",
            "Epoch [62/100] Train Loss: 0.00000477616 Test Loss: 0.00000181775\n",
            "Epoch [63/100] Train Loss: 0.00000435335 Test Loss: 0.00000162861\n",
            "Epoch [64/100] Train Loss: 0.00000434330 Test Loss: 0.00000185617\n",
            "Epoch [65/100] Train Loss: 0.00000409691 Test Loss: 0.00000174197\n",
            "Epoch [66/100] Train Loss: 0.00000376089 Test Loss: 0.00000173021\n",
            "Epoch [67/100] Train Loss: 0.00000340585 Test Loss: 0.00000153394\n",
            "Epoch [68/100] Train Loss: 0.00000336197 Test Loss: 0.00000171782\n",
            "Epoch [69/100] Train Loss: 0.00000326401 Test Loss: 0.00000155803\n",
            "Epoch [70/100] Train Loss: 0.00000292845 Test Loss: 0.00000149057\n",
            "Epoch [71/100] Train Loss: 0.00000315316 Test Loss: 0.00000146497\n",
            "Epoch [72/100] Train Loss: 0.00000303647 Test Loss: 0.00000141109\n",
            "Epoch [73/100] Train Loss: 0.00000284761 Test Loss: 0.00000154666\n",
            "Epoch [74/100] Train Loss: 0.00000273975 Test Loss: 0.00000126972\n",
            "Epoch [75/100] Train Loss: 0.00000264381 Test Loss: 0.00000137025\n",
            "Epoch [76/100] Train Loss: 0.00000249381 Test Loss: 0.00000129573\n",
            "Epoch [77/100] Train Loss: 0.00000266579 Test Loss: 0.00000143045\n",
            "Epoch [78/100] Train Loss: 0.00000236468 Test Loss: 0.00000118397\n",
            "Epoch [79/100] Train Loss: 0.00000224824 Test Loss: 0.00000102518\n",
            "Epoch [80/100] Train Loss: 0.00000222397 Test Loss: 0.00000103003\n",
            "Epoch [81/100] Train Loss: 0.00000215190 Test Loss: 0.00000103541\n",
            "Epoch [82/100] Train Loss: 0.00000217091 Test Loss: 0.00000112692\n",
            "Epoch [83/100] Train Loss: 0.00000213452 Test Loss: 0.00000109260\n",
            "Epoch [84/100] Train Loss: 0.00000218975 Test Loss: 0.00000123373\n",
            "Epoch [85/100] Train Loss: 0.00000222075 Test Loss: 0.00000105561\n",
            "Epoch [86/100] Train Loss: 0.00000205767 Test Loss: 0.00000112116\n",
            "Epoch [87/100] Train Loss: 0.00000208866 Test Loss: 0.00000102994\n",
            "Epoch [88/100] Train Loss: 0.00000185286 Test Loss: 0.00000106830\n",
            "Epoch [89/100] Train Loss: 0.00000193110 Test Loss: 0.00000096900\n",
            "Epoch [90/100] Train Loss: 0.00000188520 Test Loss: 0.00000081267\n",
            "Epoch [91/100] Train Loss: 0.00000175409 Test Loss: 0.00000086513\n",
            "Epoch [92/100] Train Loss: 0.00000177964 Test Loss: 0.00000103441\n",
            "Epoch [93/100] Train Loss: 0.00000177891 Test Loss: 0.00000091338\n",
            "Epoch [94/100] Train Loss: 0.00000167077 Test Loss: 0.00000103021\n",
            "Epoch [95/100] Train Loss: 0.00000170872 Test Loss: 0.00000089126\n",
            "Epoch [96/100] Train Loss: 0.00000161242 Test Loss: 0.00000086335\n",
            "Epoch [97/100] Train Loss: 0.00000166121 Test Loss: 0.00000074967\n",
            "Epoch [98/100] Train Loss: 0.00000165809 Test Loss: 0.00000091010\n",
            "Epoch [99/100] Train Loss: 0.00000154886 Test Loss: 0.00000085363\n",
            "Epoch [100/100] Train Loss: 0.00000156008 Test Loss: 0.00000076492\n"
          ]
        }
      ],
      "source": [
        "model_path = train_forward_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TWZSTyG15Es"
      },
      "source": [
        "### Part 2.4: Completing ArmDynamicsStudent\n",
        "\n",
        "After you are done with training, you need to complete ArmDynamicsStudent class following the comments below to load the saved checkpoint (in function init_model) and then use it to predict the new state given the current state and action (in function dynamics_step). Please do not modify the arguments to those functions, even though you might not use all of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jc0s9sdV2hN9"
      },
      "outputs": [],
      "source": [
        "from arm_dynamics_base import ArmDynamicsBase\n",
        "\n",
        "class ArmDynamicsStudent(ArmDynamicsBase):\n",
        "    def init_model(self, model_path, num_links, time_step, device):\n",
        "        # ---\n",
        "        # Your code hoes here\n",
        "        # Initialize the model loading the saved model from provided model_path\n",
        "        self.model = Model2Link(time_step)\n",
        "        self.model.load_state_dict(torch.load(model_path))\n",
        "        # ---\n",
        "        self.model_loaded = True\n",
        "\n",
        "    def dynamics_step(self, state, action, dt):\n",
        "        if self.model_loaded:\n",
        "            # ---\n",
        "            # Your code goes here\n",
        "            # Use the loaded model to predict new state given the current state and action\n",
        "          with torch.no_grad():\n",
        "                state_tensor = torch.tensor(state, dtype=torch.float32).view(1,-1)\n",
        "                action_tensor = torch.tensor(action, dtype=torch.float32).view(1,-1)\n",
        "                inputs = torch.cat((state_tensor, action_tensor), dim=1)\n",
        "                torch.reshape(inputs, (6,1))\n",
        "                op = self.model(inputs)\n",
        "                op = op.cpu().detach().numpy().reshape((4,1))\n",
        "                new_state = op\n",
        "          return new_state\n",
        "            # ---\n",
        "        else:\n",
        "            return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK686tpd3jAU"
      },
      "source": [
        "### Manually Testing the MPC Controller with the learnt dynamics model\n",
        "We will now use the learnt dynamics model that you have trained. The model is loaded in the dynamics.init_model method. You can modify the goal positions to see how well is the controller performing similar to what you did before. Feel free to play around with the code in this cell to test your performance before the grading part.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GU54TdIv3JTG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1c8e1b6-9038-48d1-fe38-d1e97db97447"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At timestep 0: Distance to goal: 2.817800560721074, Velocity of end effector: 7.715274834628325e-18\n",
            "At timestep 10: Distance to goal: 2.8192465714941113, Velocity of end effector: 0.23185132272663753\n",
            "At timestep 20: Distance to goal: 2.81609374184526, Velocity of end effector: 0.5297410652897245\n",
            "At timestep 30: Distance to goal: 2.7958825045446374, Velocity of end effector: 0.820048788453333\n",
            "At timestep 40: Distance to goal: 2.748151327776294, Velocity of end effector: 1.084574782159061\n",
            "At timestep 50: Distance to goal: 2.667338847646074, Velocity of end effector: 1.3273105320157912\n",
            "At timestep 60: Distance to goal: 2.5511427704362157, Velocity of end effector: 1.5516030708995772\n",
            "At timestep 70: Distance to goal: 2.3984437779618024, Velocity of end effector: 1.757988059244895\n",
            "At timestep 80: Distance to goal: 2.212113552487022, Velocity of end effector: 1.7594888740230457\n",
            "At timestep 90: Distance to goal: 2.0030289394822747, Velocity of end effector: 1.2906368041840124\n",
            "At timestep 100: Distance to goal: 1.7753226180268546, Velocity of end effector: 0.5183510871309035\n",
            "At timestep 110: Distance to goal: 1.505026656777721, Velocity of end effector: 0.22181239521938603\n",
            "At timestep 120: Distance to goal: 1.161943540298514, Velocity of end effector: 0.48896098215907247\n",
            "At timestep 130: Distance to goal: 0.7425904377580973, Velocity of end effector: 0.5369465396660138\n",
            "At timestep 140: Distance to goal: 0.2675356842971447, Velocity of end effector: 0.3219929121829673\n",
            "At timestep 150: Distance to goal: 0.6005934189054277, Velocity of end effector: 0.7644312501961588\n",
            "At timestep 160: Distance to goal: 1.4547631666934873, Velocity of end effector: 3.123999413105777\n",
            "At timestep 170: Distance to goal: 1.90557832272379, Velocity of end effector: 2.3017775304620725\n",
            "At timestep 180: Distance to goal: 1.9356456007144232, Velocity of end effector: 9.825902964775402\n",
            "At timestep 190: Distance to goal: 1.079983514094696, Velocity of end effector: 3.1885084834903634\n",
            "At timestep 200: Distance to goal: 1.4862506103507946, Velocity of end effector: 0.40433011791060447\n",
            "At timestep 210: Distance to goal: 2.786206238127355, Velocity of end effector: 3.9348549062267515\n",
            "At timestep 220: Distance to goal: 2.9927001154196646, Velocity of end effector: 6.158036514777655\n",
            "At timestep 230: Distance to goal: 1.7230300475265905, Velocity of end effector: 1.7859260954161749\n",
            "At timestep 240: Distance to goal: 2.1059957770504902, Velocity of end effector: 2.436513986855309\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "from arm_dynamics_teacher import ArmDynamicsTeacher\n",
        "from robot import Robot\n",
        "from render import Renderer\n",
        "from score import *\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# Teacher arm with 3 links\n",
        "dynamics_teacher = ArmDynamicsTeacher(\n",
        "    num_links=2,\n",
        "    link_mass=0.1,\n",
        "    link_length=1,\n",
        "    joint_viscous_friction=0.1,\n",
        "    dt=0.01)\n",
        "\n",
        "arm = Robot(dynamics_teacher)\n",
        "arm.reset()\n",
        "\n",
        "gui = False\n",
        "action = np.zeros((arm.dynamics.get_action_dim(), 1))\n",
        "if gui:\n",
        "  renderer = Renderer()\n",
        "  time.sleep(1)\n",
        "\n",
        "\n",
        "# Controller\n",
        "controller = MPC()\n",
        "dynamics_student = ArmDynamicsStudent(\n",
        "    num_links=2,\n",
        "    link_mass=0.1,\n",
        "    link_length=1,\n",
        "    joint_viscous_friction=0.1,\n",
        "    dt=0.01)\n",
        "device = torch.device('cpu')\n",
        "\n",
        "# model_path should have the path to the best model that you have trained so far\n",
        "# which you would like to use for testing the controller\n",
        "model_path = \"models/2023-04-16_03-52-07/epoch_0042_loss_0.00000035/dynamics.pth\"\n",
        "dynamics_student.init_model(model_path, 2, 0.01, device)\n",
        "\n",
        "# Control loop\n",
        "action = np.zeros((arm.dynamics.get_action_dim(), 1))\n",
        "goal = np.zeros((2, 1))\n",
        "goal[0, 0] = -1.3\n",
        "goal[1, 0] = 0.5\n",
        "arm.goal = goal\n",
        "\n",
        "dt = 0.01\n",
        "time_limit = 2.5\n",
        "num_steps = round(time_limit/dt)\n",
        "for s in range(num_steps):\n",
        "  t = time.time()\n",
        "  arm.advance()\n",
        "\n",
        "  if gui:\n",
        "    renderer.plot([(arm, \"tab:blue\")])\n",
        "  time.sleep(max(0, dt - (time.time() - t)))\n",
        "\n",
        "  if s % controller.control_horizon==0:\n",
        "    state = arm.get_state()\n",
        "    pos_ee = dynamics_teacher.compute_fk(state)\n",
        "    dist = np.linalg.norm(goal-pos_ee)\n",
        "    vel_ee = np.linalg.norm(arm.dynamics.compute_vel_ee(state))\n",
        "    print(f'At timestep {s}: Distance to goal: {dist}, Velocity of end effector: {vel_ee}')\n",
        "    action = controller.compute_action(dynamics_student, state, goal, action)\n",
        "    arm.set_action(action)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grading and Evaluation of Part 2\n",
        "You will be evaluated on how well your controller+learnt dynamics works together. The scoring functions consists of 16 random test goals all of which will be below the x axis and between 0.05 to 1.95 lengths away from the origin.\n",
        "The controller will call the compute_action method from your MPC class and apply the action for 10 timesteps\n",
        "```\n",
        "action = controller.compute_action(dynamics_student, state, goal, action)\n",
        "```\n",
        "\n",
        "Each test will run the robot arm for **2.5 seconds**. At the end of the 2.5 seconds the test will be:\n",
        "\n",
        "A success if your end effectors meet this criteria:\n",
        "`distance_to_goal < 0.2 and vel_ee < 0.5`\n",
        "\n",
        "A partial success if your end effectors meet this criteria:\n",
        "`distance_to_goal < 0.3 and vel_ee < 0.5`\n",
        "\n",
        "You need 15 out of the 16 tests to succeed to get a full score"
      ],
      "metadata": {
        "id": "tn1RPPnLxrpf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B28gYRyYEgrz"
      },
      "outputs": [],
      "source": [
        "controller = MPC()\n",
        "dynamics_student = ArmDynamicsStudent(\n",
        "    num_links=2,\n",
        "    link_mass=0.1,\n",
        "    link_length=1,\n",
        "    joint_viscous_friction=0.1,\n",
        "    dt=0.01)\n",
        "model_path = 'models/2023-04-16_03-52-07/epoch_0042_loss_0.00000035/dynamics.pth'\n",
        "gui=False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE\n",
        "score_mpc_learnt_dynamics(controller, dynamics_student, model_path, gui)"
      ],
      "metadata": {
        "id": "JgmrICx-1MGg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "outputId": "0c89b713-2d1c-4770-bf9e-f8babcf27dc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part2: EVALUATING CONTROLLER + LEARNED DYNAMICS\n",
            "-----------------------------------------------\n",
            "NUM_LINKS: 2\n",
            "Test  1\n",
            "Fail :(\n",
            " Goal: [-0.83931506 -0.17109352], Final position: [-0.59318193 -1.33531289], Final velocity: [0.08169862]\n",
            "score: 0/0.5\n",
            "Test  2\n",
            "Fail :(\n",
            " Goal: [ 0.27206229 -0.1323684 ], Final position: [-0.8017003  0.0386893], Final velocity: [0.62254448]\n",
            "score: 0/0.5\n",
            "Test  3\n",
            "Fail :(\n",
            " Goal: [-0.53098054 -0.98812195], Final position: [-0.08675118  0.07278706], Final velocity: [0.33553007]\n",
            "score: 0/0.5\n",
            "Test  4\n",
            "Fail :(\n",
            " Goal: [ 0.20637967 -0.28259279], Final position: [-1.10665201 -0.85855824], Final velocity: [0.19507219]\n",
            "score: 0/0.5\n",
            "Test  5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-228-383dcc46ac78>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# DO NOT CHANGE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscore_mpc_learnt_dynamics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontroller\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamics_student\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgui\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/score.py\u001b[0m in \u001b[0;36mscore_mpc_learnt_dynamics\u001b[0;34m(controller, arm_student, model_path, gui)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                 result, pos_ee, vel_ee = test(arm, dynamics, goal, renderer, \\\n\u001b[0m\u001b[1;32m    210\u001b[0m                           controller, gui, args, dist_limit=[0.2, 0.3], time_limit=2.5)\n\u001b[1;32m    211\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/score.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(arm, dynamics, goal, renderer, controller, gui, args, dist_limit, time_limit)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcontroller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_horizon\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontroller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdynamics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0marm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0marm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-213-991348a88075>\u001b[0m in \u001b[0;36mcompute_action\u001b[0;34m(self, dynamics, state, goal, action)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m#new_state_up = new_state_up.detach().cpu().numpy().reshape((len(original_state),1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mpos_up\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdynamics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_fk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state_up\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mnew_state_down\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdynamics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamics_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_state_down\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_horizon_down\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0;31m#new_state_down = new_state_down.detach().cpu().numpy().reshape((len(original_state),1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mpos_down\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdynamics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_fk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state_down\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-224-c99ea99cb250>\u001b[0m in \u001b[0;36mdynamics_step\u001b[0;34m(self, state, action, dt)\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                 \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-220-95748d096db0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0mqddot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_qddot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_links\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_next_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqddot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-220-95748d096db0>\u001b[0m in \u001b[0;36mcompute_qddot\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m                         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1603\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_parameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_buffers'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m             \u001b[0m_buffers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_buffers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5lIxv47dgEm"
      },
      "source": [
        "Hints and Suggestions:\n",
        "1. You can use your MPC Controller in your data collection to gather better training samples\n",
        "2. A good cost function to evaluate your trajectory in MPC is very important and you can use both distance and velocity metrics to define the cost function.\n",
        "3. As mentioned in the lecture, a constant torque with pseudo gradients seems to work well for this project. You can also use multiple delta values to gather more trajectories to choose from.\n",
        "4. Since we are passing the MPC object to the controller you can instantiate the MPC class with different parameters like the planning horizon, delta values etc.\n",
        "5. To speed up data collection, avoid using np.concatenate(), np.stack() or np.append() like functions on your X and Y arrays. Instead, initialize X and Y arrays with all zeros using the correct shape and then fill in the values one by one. This is much faster in numpy"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}